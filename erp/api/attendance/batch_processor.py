"""
Attendance Batch Processor
Xá»­ lÃ½ batch attendance events tá»« Redis buffer

PERFORMANCE:
- Scheduled job cháº¡y má»—i 5 giÃ¢y (hoáº·c cÃ³ thá»ƒ trigger thá»§ cÃ´ng)
- Xá»­ lÃ½ 100 events má»—i láº§n trong 1 transaction
- Giáº£m 90% database load so vá»›i xá»­ lÃ½ tá»«ng event
- Notification Ä‘Æ°á»£c batch enqueue

Author: System
Created: 2026-01-20
"""

import frappe
from frappe import _
import json
from datetime import datetime
from collections import defaultdict
import pytz
import logging
import os

from erp.api.attendance.hikvision import (
	ATTENDANCE_BUFFER_KEY,
	BUFFER_BATCH_SIZE,
	pop_from_attendance_buffer,
	get_buffer_length,
	parse_attendance_timestamp,
	format_vn_time,
	is_historical_attendance,
	get_historical_attendance_threshold_minutes,
	get_hikvision_logger
)
from erp.common.doctype.erp_time_attendance.erp_time_attendance import (
	find_or_create_day_record,
	normalize_date_to_vn_timezone
)


def get_batch_processor_logger():
	"""Get or create batch processor logger vá»›i file handler riÃªng"""
	logger = logging.getLogger('attendance_batch_processor')
	
	if not logger.handlers:
		logger.setLevel(logging.DEBUG)
		
		log_dir = frappe.get_site_path('logs')
		if not os.path.exists(log_dir):
			os.makedirs(log_dir)
		
		log_file = os.path.join(log_dir, 'attendance_batch_processor.log')
		file_handler = logging.FileHandler(log_file)
		file_handler.setLevel(logging.DEBUG)
		
		formatter = logging.Formatter(
			'%(asctime)s - [BATCH_PROCESSOR] - %(levelname)s - %(message)s',
			datefmt='%Y-%m-%d %H:%M:%S'
		)
		file_handler.setFormatter(formatter)
		
		logger.addHandler(file_handler)
		logger.propagate = False
	
	return logger


@frappe.whitelist()
def process_attendance_buffer():
	"""
	Xá»­ lÃ½ batch attendance events tá»« Redis buffer.
	ÄÆ°á»£c gá»i bá»Ÿi scheduler má»—i 5 giÃ¢y.
	
	Flow:
	1. Pop batch events tá»« Redis buffer
	2. Group events theo employee_code + date
	3. Batch upsert vÃ o database trong 1 transaction
	4. Batch enqueue notifications
	"""
	logger = get_batch_processor_logger()
	
	try:
		# Check buffer length
		buffer_length = get_buffer_length()
		
		if buffer_length == 0:
			# KhÃ´ng log náº¿u buffer rá»—ng Ä‘á»ƒ trÃ¡nh spam
			return {
				"status": "success",
				"message": "Buffer is empty",
				"processed": 0
			}
		
		logger.info("=" * 80)
		logger.info(f"ðŸš€ Starting batch processing - {buffer_length} events in buffer")
		
		# Pop batch tá»« buffer
		events = pop_from_attendance_buffer(BUFFER_BATCH_SIZE)
		
		if not events:
			logger.info("No events to process after pop")
			return {
				"status": "success",
				"message": "No events to process",
				"processed": 0
			}
		
		logger.info(f"ðŸ“¥ Popped {len(events)} events from buffer")
		
		# Group events theo employee_code + date
		grouped_events = group_events_by_employee_date(events)
		
		logger.info(f"ðŸ“Š Grouped into {len(grouped_events)} employee-date combinations")
		
		# OPTIMIZATION: Batch query existing records trÆ°á»›c Ä‘á»ƒ giáº£m N queries xuá»‘ng 1
		existing_map = batch_get_existing_records(grouped_events, logger)
		logger.info(f"ðŸ“‹ Found {len(existing_map)} existing records")
		
		# Process each group
		records_processed = 0
		records_updated = 0
		errors = []
		notification_queue = []
		
		# Báº¯t Ä‘áº§u transaction
		try:
			for key, employee_events in grouped_events.items():
				try:
					# Truyá»n existing record name náº¿u cÃ³
					existing_name = existing_map.get(key)
					result = process_employee_events(key, employee_events, logger, existing_name=existing_name)
					
					if result.get("success"):
						if result.get("is_new"):
							records_processed += 1
						else:
							records_updated += 1
						
						# ThÃªm vÃ o notification queue náº¿u cáº§n
						if result.get("should_notify"):
							notification_queue.append(result.get("notification_data"))
					else:
						errors.append({
							"key": key,
							"error": result.get("error")
						})
						
				except Exception as emp_error:
					logger.error(f"âŒ Error processing {key}: {str(emp_error)}")
					errors.append({
						"key": key,
						"error": str(emp_error)
					})
			
			# Commit táº¥t cáº£ trong 1 transaction
			if records_processed > 0 or records_updated > 0:
				frappe.db.commit()
				logger.info(f"ðŸ’¾ Batch committed: {records_processed} new, {records_updated} updated")
			
		except Exception as tx_error:
			frappe.db.rollback()
			logger.error(f"âŒ Transaction error, rolling back: {str(tx_error)}")
			raise
		
		# Batch enqueue notifications (sau khi commit)
		notifications_sent = 0
		for notif_data in notification_queue:
			try:
				enqueue_attendance_notification(notif_data)
				notifications_sent += 1
			except Exception as notif_error:
				logger.warning(f"âš ï¸ Failed to enqueue notification: {str(notif_error)}")
		
		logger.info(f"ðŸ“Š BATCH SUMMARY: {records_processed} new, {records_updated} updated, {notifications_sent} notifications, {len(errors)} errors")
		logger.info("=" * 80)
		
		return {
			"status": "success",
			"message": f"Processed {records_processed + records_updated} attendance records",
			"records_processed": records_processed,
			"records_updated": records_updated,
			"notifications_sent": notifications_sent,
			"total_errors": len(errors),
			"remaining_in_buffer": get_buffer_length()
		}
		
	except Exception as e:
		logger.error(f"âŒ FATAL ERROR in batch processor: {str(e)}")
		frappe.log_error(message=str(e), title="Attendance Batch Processor Error")
		return {
			"status": "error",
			"message": str(e),
			"processed": 0
		}


def group_events_by_employee_date(events):
	"""
	Group events theo employee_code + date.
	Nhiá»u events cÃ¹ng employee + date sáº½ Ä‘Æ°á»£c merge thÃ nh 1 record.
	
	Args:
		events: List of event dicts
	
	Returns:
		Dict: {(employee_code, date_str): [events]}
	"""
	grouped = defaultdict(list)
	
	for event in events:
		try:
			employee_code = event.get("employee_code")
			timestamp = event.get("timestamp")
			
			if not employee_code or not timestamp:
				continue
			
			# Parse timestamp Ä‘á»ƒ láº¥y date
			parsed_ts = parse_attendance_timestamp(timestamp)
			date_str = parsed_ts.strftime('%Y-%m-%d')
			
			# Add parsed timestamp vÃ o event
			event["parsed_timestamp"] = parsed_ts
			
			key = (employee_code, date_str)
			grouped[key].append(event)
			
		except Exception as e:
			get_batch_processor_logger().warning(f"âš ï¸ Error grouping event: {str(e)}")
			continue
	
	return grouped


def batch_get_existing_records(grouped_events, logger):
	"""
	Batch query Ä‘á»ƒ láº¥y táº¥t cáº£ existing attendance records trong 1 query.
	Giáº£m tá»« N queries xuá»‘ng cÃ²n 1.
	
	Args:
		grouped_events: Dict {(employee_code, date_str): events}
		logger: Logger instance
	
	Returns:
		Dict: {(employee_code, date_str): record_name}
	"""
	if not grouped_events:
		return {}
	
	# Extract unique employee_codes vÃ  dates
	employee_codes = list(set([key[0] for key in grouped_events.keys()]))
	dates = list(set([key[1] for key in grouped_events.keys()]))
	
	if not employee_codes or not dates:
		return {}
	
	try:
		# Single batch query cho táº¥t cáº£ combinations
		existing_records = frappe.db.sql("""
			SELECT name, employee_code, date 
			FROM `tabERP Time Attendance`
			WHERE employee_code IN %(employee_codes)s
			AND date IN %(dates)s
		""", {
			"employee_codes": employee_codes,
			"dates": dates
		}, as_dict=True)
		
		# Build lookup map
		result = {}
		for rec in existing_records:
			key = (rec.employee_code, str(rec.date))
			result[key] = rec.name
		
		return result
		
	except Exception as e:
		logger.error(f"âŒ Error batch querying existing records: {str(e)}")
		return {}


def process_employee_events(key, events, logger, existing_name=None):
	"""
	Xá»­ lÃ½ táº¥t cáº£ events cho 1 employee trong 1 ngÃ y.
	
	Args:
		key: Tuple (employee_code, date_str)
		events: List of events cho employee nÃ y
		logger: Logger instance
		existing_name: Optional - tÃªn record náº¿u Ä‘Ã£ tá»“n táº¡i (tá»« batch query)
	
	Returns:
		Dict with processing result
	"""
	employee_code, date_str = key
	
	# Láº¥y thÃ´ng tin tá»« event Ä‘áº§u tiÃªn (hoáº·c merge tá»« táº¥t cáº£)
	first_event = events[0]
	employee_name = None
	device_id = None
	device_name = None
	
	# TÃ¬m employee_name tá»« events (Æ°u tiÃªn event cÃ³ name)
	for evt in events:
		if evt.get("employee_name"):
			employee_name = evt.get("employee_name")
		if evt.get("device_id"):
			device_id = evt.get("device_id")
		if evt.get("device_name"):
			device_name = evt.get("device_name")
	
	# Láº¥y timestamp sá»›m nháº¥t lÃ m reference
	sorted_events = sorted(events, key=lambda x: x.get("parsed_timestamp"))
	earliest_event = sorted_events[0]
	earliest_timestamp = earliest_event.get("parsed_timestamp")
	
	logger.debug(f"Processing {len(events)} events for {employee_code} on {date_str}")
	
	try:
		# OPTIMIZED: DÃ¹ng existing_name náº¿u cÃ³, khÃ´ng cáº§n query láº¡i
		if existing_name:
			# Load existing record directly
			attendance_doc = frappe.get_doc("ERP Time Attendance", existing_name)
			# Update employee_name vÃ  device_name náº¿u provided vÃ  chÆ°a cÃ³
			if employee_name and not attendance_doc.employee_name:
				attendance_doc.employee_name = employee_name
			if device_name and not attendance_doc.device_name:
				attendance_doc.device_name = device_name
			is_new = False
		else:
			# Create new record
			attendance_doc = frappe.new_doc("ERP Time Attendance")
			attendance_doc.employee_code = employee_code
			attendance_doc.employee_name = employee_name
			attendance_doc.date = normalize_date_to_vn_timezone(earliest_timestamp)
			attendance_doc.device_id = device_id
			attendance_doc.device_name = device_name
			attendance_doc.raw_data = "[]"
			is_new = True
		
		# Update vá»›i táº¥t cáº£ timestamps tá»« events
		for evt in sorted_events:
			parsed_ts = evt.get("parsed_timestamp")
			evt_device_id = evt.get("device_id") or device_id
			evt_device_name = evt.get("device_name") or device_name
			original_timestamp = evt.get("timestamp")
			
			attendance_doc.update_attendance_time(
				parsed_ts, 
				evt_device_id, 
				evt_device_name, 
				original_timestamp=original_timestamp
			)
		
		# Add notes tá»« events
		notes_parts = []
		for evt in events:
			if evt.get("face_id_name"):
				notes_parts.append(f"Face: {evt.get('face_id_name')}")
			if evt.get("similarity"):
				notes_parts.append(f"Sim: {evt.get('similarity')}%")
		
		if notes_parts:
			existing_notes = attendance_doc.notes or ""
			# Chá»‰ thÃªm náº¿u chÆ°a cÃ³
			new_note = "; ".join(notes_parts[:5]) + "; "  # Limit to 5 entries
			if new_note not in existing_notes:
				attendance_doc.notes = existing_notes + new_note
		
		# Save (khÃ´ng commit - sáº½ commit batch sau)
		attendance_doc.save(ignore_permissions=True)
		
		logger.info(f"âœ… {employee_name or employee_code} - {len(events)} events merged - check_in: {format_vn_time(attendance_doc.check_in_time)}, check_out: {format_vn_time(attendance_doc.check_out_time)}")
		
		# Determine if should send notification
		should_notify = False
		latest_timestamp = sorted_events[-1].get("parsed_timestamp")
		
		if not is_historical_attendance(latest_timestamp):
			should_notify = True
		
		notification_data = None
		if should_notify:
			notification_data = {
				"employee_code": employee_code,
				"employee_name": employee_name,
				"timestamp": latest_timestamp.isoformat(),
				"device_id": device_id,
				"device_name": device_name,
				"check_in_time": attendance_doc.check_in_time.isoformat() if attendance_doc.check_in_time else None,
				"check_out_time": attendance_doc.check_out_time.isoformat() if attendance_doc.check_out_time else None,
				"total_check_ins": attendance_doc.total_check_ins,
				"date": str(attendance_doc.date)
			}
		
		return {
			"success": True,
			"is_new": is_new,
			"should_notify": should_notify,
			"notification_data": notification_data
		}
		
	except Exception as e:
		logger.error(f"âŒ Error processing {employee_code}: {str(e)}")
		return {
			"success": False,
			"error": str(e)
		}


def enqueue_attendance_notification(notif_data):
	"""
	Enqueue notification job.
	
	Args:
		notif_data: Dict with notification data
	"""
	employee_code = notif_data.get("employee_code")
	timestamp = notif_data.get("timestamp")
	
	# Parse timestamp for job_id
	if isinstance(timestamp, str):
		ts = frappe.utils.get_datetime(timestamp)
	else:
		ts = timestamp
	
	frappe.enqueue(
		"erp.api.attendance.notification.publish_attendance_notification",
		queue="short",
		job_id=f"attendance_notif_{employee_code}_{ts.strftime('%H%M%S')}",
		deduplicate=True,
		timeout=120,
		**notif_data
	)


@frappe.whitelist()
def trigger_batch_processing():
	"""
	API Ä‘á»ƒ trigger batch processing thá»§ cÃ´ng.
	DÃ¹ng cho testing hoáº·c khi cáº§n xá»­ lÃ½ ngay.
	"""
	return process_attendance_buffer()


@frappe.whitelist()
def get_processor_stats():
	"""
	API Ä‘á»ƒ láº¥y statistics cá»§a batch processor.
	"""
	try:
		buffer_length = get_buffer_length()
		
		return {
			"status": "success",
			"stats": {
				"pending_events": buffer_length,
				"buffer_key": ATTENDANCE_BUFFER_KEY,
				"batch_size": BUFFER_BATCH_SIZE,
				"processing_interval": "5 seconds"
			},
			"timestamp": frappe.utils.now()
		}
	except Exception as e:
		return {
			"status": "error",
			"message": str(e)
		}


@frappe.whitelist()
def clear_buffer():
	"""
	API Ä‘á»ƒ clear buffer (CHá»ˆ DÃ™NG CHO TESTING/EMERGENCY).
	Cáº§n permission System Manager.
	"""
	if not frappe.has_permission("System Manager"):
		frappe.throw(_("Not permitted"), frappe.PermissionError)
	
	try:
		cache = frappe.cache()
		
		# Delete the buffer key
		if hasattr(cache, 'delete_value'):
			cache.delete_value(ATTENDANCE_BUFFER_KEY)
		else:
			redis_conn = cache.redis if hasattr(cache, 'redis') else None
			if redis_conn:
				redis_conn.delete(ATTENDANCE_BUFFER_KEY)
		
		return {
			"status": "success",
			"message": "Buffer cleared",
			"timestamp": frappe.utils.now()
		}
	except Exception as e:
		return {
			"status": "error",
			"message": str(e)
		}


# Scheduled job function - Ä‘Æ°á»£c gá»i bá»Ÿi hooks.py
def scheduled_process_attendance_buffer():
	"""
	Scheduled job wrapper cho process_attendance_buffer.
	ÄÆ°á»£c hooks.py gá»i má»—i 5 giÃ¢y.
	"""
	try:
		result = process_attendance_buffer()
		
		# Log náº¿u cÃ³ xá»­ lÃ½ gÃ¬ Ä‘Ã³
		if result.get("records_processed", 0) > 0 or result.get("records_updated", 0) > 0:
			frappe.logger().info(f"[Attendance Batch] Processed: {result}")
		
		return result
		
	except Exception as e:
		frappe.logger().error(f"[Attendance Batch] Error: {str(e)}")
		frappe.log_error(message=str(e), title="Scheduled Attendance Batch Error")
		return {"status": "error", "message": str(e)}
